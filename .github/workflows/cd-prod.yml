name: CD - Deploy to Production

on:
  workflow_dispatch:  # Manual trigger only for production
    inputs:
      confirm_deploy:
        description: 'Type "deploy-to-prod" to confirm'
        required: true
        default: ''

# PRODUCTION DEPLOYMENT STRATEGY:
# - Manual trigger only (no auto-deploy)
# - Requires typing "deploy-to-prod" to confirm
# - Requires GitHub environment approval
# - Deploy Docker image, infrastructure, and Kubernetes manifests

env:
  AWS_REGION: eu-central-1
  ECR_REPOSITORY: yambo-prod-spark-job
  EKS_CLUSTER_NAME: yambo-prod-eks

jobs:
  validate-input:
    name: Validate Deployment Input
    runs-on: ubuntu-latest
    
    steps:
      - name: Check confirmation
        run: |
          if [ "${{ github.event.inputs.confirm_deploy }}" != "deploy-to-prod" ]; then
            echo "❌ Confirmation text does not match. Aborting deployment."
            exit 1
          fi
          echo "✅ Deployment confirmed"
  
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    needs: [validate-input]
    environment: production
    
    outputs:
      image-tag: ${{ steps.meta.outputs.version }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_PROD }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
      
      - name: Docker meta
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}
          tags: |
            type=sha,prefix=prod-,format=short
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=raw,value=prod-latest
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
  
  deploy-infrastructure:
    name: Deploy Infrastructure (Terraform)
    runs-on: ubuntu-latest
    needs: build-and-push
    environment: production
    
    outputs:
      irsa_role_arn: ${{ steps.terraform-outputs.outputs.irsa_arn }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_PROD }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
      
      - name: Package Lambda function
        run: |
          python scripts/package-lambda.py
      
      - name: Terraform init
        run: |
          cd terraform/environments/prod
          terraform init
      
      - name: Terraform plan
        run: |
          cd terraform/environments/prod
          terraform plan -out=tfplan
      
      - name: Terraform apply
        run: |
          cd terraform/environments/prod
          terraform apply -auto-approve tfplan
      
      - name: Get Terraform outputs
        id: terraform-outputs
        run: |
          cd terraform/environments/prod
          IRSA_ARN=$(terraform output -raw spark_job_irsa_role_arn)
          echo "irsa_arn=$IRSA_ARN" >> $GITHUB_OUTPUT
  
  deploy-kubernetes:
    name: Deploy Kubernetes Manifests
    runs-on: ubuntu-latest
    needs: [build-and-push, deploy-infrastructure]
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_PROD }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
      
      - name: Update service account with IRSA
        run: |
          kubectl annotate serviceaccount spark-job-sa \
            -n spark-jobs \
            eks.amazonaws.com/role-arn=${{ needs.deploy-infrastructure.outputs.irsa_role_arn }} \
            --overwrite
      
      - name: Apply Kubernetes manifests
        run: |
          kubectl apply -f k8s/namespace.yaml
          kubectl apply -f k8s/service-account.yaml
          kubectl apply -f k8s/rbac.yaml
          
          # Update ConfigMap for prod environment
          sed -i 's/ENVIRONMENT: "dev"/ENVIRONMENT: "prod"/' k8s/config-map.yaml
          sed -i 's/-dev/-prod/g' k8s/config-map.yaml
          kubectl apply -f k8s/config-map.yaml
      
      - name: Update SparkApplication image
        run: |
          IMAGE_TAG=${{ needs.build-and-push.outputs.image-tag }}
          ECR_URI="${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_REPOSITORY }}:${IMAGE_TAG}"
          
          # Update check job
          sed -i "s|image:.*|image: \"${ECR_URI}\"|" k8s/spark-application-check.yaml
          kubectl apply -f k8s/spark-application-check.yaml
          
          # Update extract job
          sed -i "s|image:.*|image: \"${ECR_URI}\"|" k8s/spark-application-extract.yaml
          kubectl apply -f k8s/spark-application-extract.yaml
  
  smoke-test:
    name: Run Smoke Tests
    runs-on: ubuntu-latest
    needs: deploy-kubernetes
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_PROD }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
      
      - name: Run check job
        continue-on-error: true
        run: |
          # Delete existing job if it exists (fresh run)
          kubectl delete sparkapplication yambo-check-job -n spark-jobs --ignore-not-found=true
          
          # Create new check job
          kubectl create -f k8s/spark-application-check.yaml
          
          # Wait for job to start (30 sec)
          sleep 30
          
          # Show job status
          kubectl describe sparkapplication yambo-check-job -n spark-jobs || true
          
          # Wait for job completion (2 min timeout - just for smoke test)
          kubectl wait --for=condition=Complete \
            sparkapplication/yambo-check-job \
            -n spark-jobs \
            --timeout=120s || echo "Check job did not complete in time - this is OK for deployment"
      
      - name: Get check job status
        if: always()
        run: |
          echo "=== SparkApplication Status ==="
          kubectl get sparkapplication yambo-check-job -n spark-jobs -o yaml || true
          
          echo ""
          echo "=== Driver Pod Status ==="
          kubectl get pods -n spark-jobs -l spark-role=driver --sort-by=.metadata.creationTimestamp | tail -5
      
      - name: Get check job logs
        if: always()
        run: |
          echo "=== Driver Logs (last 50 lines) ==="
          DRIVER_POD=$(kubectl get pods -n spark-jobs -l spark-role=driver,app=yambo-spark-job --sort-by=.metadata.creationTimestamp -o name | tail -1)
          if [ -n "$DRIVER_POD" ]; then
            kubectl logs -n spark-jobs $DRIVER_POD --tail=50 || echo "No logs available yet"
          else
            echo "No driver pod found"
          fi
      
      - name: Send deployment notification
        if: always()
        run: |
          echo "Production deployment completed"
          # Send to Slack, PagerDuty, etc.

# PRODUCTION DEPLOYMENT NOTES:
#
# This workflow deploys to production when:
# - Code is pushed to main branch
# - A release is published
# - Manually triggered
#
# REQUIRED SECRETS:
# - AWS_ROLE_ARN_PROD: IAM role ARN for production
# - AWS_ACCOUNT_ID: AWS account ID
#
# SAFETY FEATURES:
# - GitHub environment protection (manual approval required)
# - Terraform plan and apply separated
# - Smoke tests before marking deployment complete
# - Rollback capability via kubectl
#
# ROLLBACK PROCEDURE:
# 1. Identify last working image tag
# 2. Update SparkApplication manifests with previous tag
# 3. kubectl apply -f k8s/spark-application-*.yaml
#
# MONITORING:
# - Check CloudWatch logs for job execution
# - Monitor S3 for data landing
# - Check Spark UI for job status
# - Alert on job failures
