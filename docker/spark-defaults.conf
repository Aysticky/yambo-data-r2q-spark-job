# Spark configuration defaults
# These can be overridden at runtime via spark-submit

# Application Configuration
spark.app.name=yambo-data-pipeline

# Resource Configuration
spark.executor.memory=4g
spark.executor.cores=2
spark.driver.memory=2g
spark.driver.cores=1

# Dynamic Resource Allocation
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=10
spark.dynamicAllocation.initialExecutors=2

# Shuffle Configuration
spark.sql.shuffle.partitions=200
spark.shuffle.service.enabled=false

# S3 Configuration
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.multipart.size=104857600
spark.hadoop.fs.s3a.connection.maximum=100

# Parquet Configuration
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.mergeSchema=false
spark.sql.parquet.filterPushdown=true

# Memory Management
spark.memory.fraction=0.6
spark.memory.storageFraction=0.5

# Logging
spark.eventLog.enabled=true
spark.eventLog.dir=s3a://yambo-spark-logs-dev/event-logs

# K8s Configuration (overridden at runtime)
spark.kubernetes.namespace=spark-jobs
spark.kubernetes.driver.pod.name=yambo-driver
spark.kubernetes.executor.deleteOnTermination=true

# UI Configuration
spark.ui.enabled=true
spark.ui.port=4040

# Serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max=512m

# Network
spark.network.timeout=300s
spark.executor.heartbeatInterval=30s

# Checkpointing
spark.cleaner.referenceTracking.cleanCheckpoints=true
